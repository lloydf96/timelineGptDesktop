# -*- coding: utf-8 -*-
"""data_extract.py

Created by Praveen Kumar Murugaiah, parveen00@gmail.com
Automatically generated by Colaboratory.

"""

# Import all the necessary packages

import requests
import re
import pandas as pd
from bs4 import BeautifulSoup
import numpy as np
import warnings
warnings.filterwarnings('ignore')

'''
Google search link function takes in the search_term we need to search for and 
  then returns the first wikipedia link from the google search
'''

def get_wiki_link(search_term):
  url = f'https://en.wikipedia.org/w/index.php?search={search_term}&title=Special:Search&profile=advanced&fulltext=1&ns0=1'
  page = requests.get(url)
  soup = BeautifulSoup(page.content, "html.parser")
  
  links = []
  main_url = 'https://en.wikipedia.org'
  for link in soup.findAll('a'):
    links.append(link['href'])
    
  if 'redlink' in links[30]:
    if 'offset' in str(links[32]):
      for link in links[32:]:
        if '/wiki/' in link:
          return main_url + link
    else:
      return 'NONE'
  elif '/wiki/' in links[30]:
    return main_url + links[30]
    
  if 'Special:Search' in links[30]:
    for link in links[31:]:
      if '/wiki/' in link and 'Article_wizard' not in link:
        return main_url + link


# Get the content from Wikipedia
def get_wikipedia_text(link):
  page = requests.get(link)
  soup = BeautifulSoup(page.content, "html.parser")
  final_texts = []
  for p in soup.findAll('p'):
    text = p.text.strip()
    text = re.sub(r'[[0-9]+]', '', text)
    final_texts.append(text)
  final_text = ' '.join(final_texts).strip().replace("\'s","'s")
  return final_text


def separate_words(text):
    separated_text = re.sub(r'([a-z0-9])([A-Z0-9])', r'\1 \2', text)
    return separated_text

    
def validate_url(url_link):
    '''
    This function checks if the input url is a valid url or not. 
        - If valid, returns the scarped text from the url. 
        - If invalid, it returns a message displaying that link is invalid
    '''
    try:
        response = requests.get(url_link)
        if str(response) == '<Response [200]>':
            soup = BeautifulSoup(response.content, "html.parser")
            s = separate_words(soup.text.replace('\n',''))
            return s, 'Correct Link'
    except:
        
        return 'No Text', 'Invalid Link'


# Since our chatGPT api can work with max 90,000 tokens per minute, we will cap the number of tokens in our wikipedia text

