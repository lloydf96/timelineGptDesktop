{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd7218d-6927-49f5-b4a9-a7bebfc318b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary packages\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''\n",
    "Google search link function takes in the search_term we need to search for and \n",
    "  then returns the first wikipedia link from the google search\n",
    "'''\n",
    "\n",
    "def get_wiki_link(search_term):\n",
    "  url = f'https://en.wikipedia.org/w/index.php?search={search_term}&title=Special:Search&profile=advanced&fulltext=1&ns0=1'\n",
    "  page = requests.get(url)\n",
    "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "  \n",
    "  links = []\n",
    "  main_url = 'https://en.wikipedia.org/'\n",
    "  for link in soup.findAll('a'):\n",
    "    links.append(link['href'])\n",
    "    \n",
    "  if 'redlink' in links[30]:\n",
    "    return 'NONE'\n",
    "  elif '/wiki/' in links[30]:\n",
    "    return links[30]\n",
    "    \n",
    "  if 'Special:Search' in links[30]:\n",
    "    for link in links[31:]:\n",
    "      if '/wiki/' in link and 'Article_wizard' not in link:\n",
    "        return link\n",
    "\n",
    "\n",
    "# Get the content from Wikipedia\n",
    "def get_wikipedia_text(link):\n",
    "  page = requests.get(link)\n",
    "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "  final_texts = []\n",
    "  for p in soup.findAll('p'):\n",
    "    text = p.text.strip()\n",
    "    text = re.sub(r'[[0-9]+]', '', text)\n",
    "    final_texts.append(text)\n",
    "  final_text = ' '.join(final_texts).strip().replace(\"\\'s\",\"'s\")\n",
    "  return final_text\n",
    "\n",
    "  # Since our chatGPT api can work with max 90,000 tokens per minute, we will cap the number of tokens in our wikipedia text\n",
    "\n",
    "#if wikipedia_link == 'NONE':\n",
    "#final_text = 'NONE'\n",
    "#else:\n",
    "#  final_text = get_wikipedia_text(wikipedia_link)\n",
    "\n",
    "# # Save the text to file\n",
    "# text_file = open(\"output.txt\", \"w\")\n",
    "# text_file.write(final_text)\n",
    "# text_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0dec64f-db06-44fe-ab2d-95c1d91e03cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NONE'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wiki_link('bmc software')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b60ef-95fa-428f-a84c-2c56f5703d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
